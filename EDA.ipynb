{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de59fa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aace57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('/Users/shubanborkar/Documents/GitHub/Carbon-Footprint-Prediction/emissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c76ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate data and clean up\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432a8ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Print dataset info\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Unique companies:\", len(df['CompanyName'].unique()))\n",
    "print(\"Unique years:\", len(df['Year'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80d14e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create one-hot encoding function\n",
    "def one_hot_encode(df, column):\n",
    "    unique_values = df[column].unique()\n",
    "    result = {}\n",
    "    for i, value in enumerate(unique_values):\n",
    "        # Skip first value for reference (to avoid perfect collinearity)\n",
    "        if i == 0:\n",
    "            continue\n",
    "        result[f\"{column}_{value}\"] = (df[column] == value).astype(float)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c00954",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create our standardizer class\n",
    "class Standardizer:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def fit(self, data):\n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        self.std = np.std(data, axis=0)\n",
    "        # Prevent division by zero\n",
    "        self.std = np.where(self.std == 0, 1, self.std)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "    \n",
    "    def inverse_transform(self, data):\n",
    "        return data * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160de702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one-hot encoded features\n",
    "categorical_cols = ['CompanyName', 'Sector', 'Location']\n",
    "numerical_cols = ['Year', 'Electricity', 'FossilFuels', 'Renewables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each categorical column\n",
    "encoded_features = {}\n",
    "for col in categorical_cols:\n",
    "    encoded_features.update(one_hot_encode(df, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed dataframe\n",
    "df_processed = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add encoded categorical features\n",
    "for col_name, col_data in encoded_features.items():\n",
    "    df_processed[col_name] = col_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c406338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add numerical features\n",
    "for col in numerical_cols:\n",
    "    df_processed[col] = df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e91d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variables\n",
    "y = df[['CO2', 'CH4', 'N2O']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106f331",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Feature matrix\n",
    "X = df_processed.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b581e2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Implement train-test split\n",
    "def train_test_split(X, y, test_size=0.2, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    indices = np.random.permutation(len(X))\n",
    "    test_size_count = int(len(X) * test_size)\n",
    "    test_indices = indices[:test_size_count]\n",
    "    train_indices = indices[test_size_count:]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler_X = Standardizer()\n",
    "X_train = scaler_X.fit(X_train).transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize targets\n",
    "scaler_y = Standardizer()\n",
    "y_train = scaler_y.fit(y_train).transform(y_train)\n",
    "y_test = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f09a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 24\n",
    "hidden_size2 = 12\n",
    "output_size = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f594b63",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Xavier/Glorot initialization\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size1) * np.sqrt(2 / (input_size + hidden_size1))\n",
    "b1 = np.zeros((1, hidden_size1))\n",
    "W2 = np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2 / (hidden_size1 + hidden_size2))\n",
    "b2 = np.zeros((1, hidden_size2))\n",
    "W3 = np.random.randn(hidden_size2, output_size) * np.sqrt(2 / (hidden_size2 + output_size))\n",
    "b3 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d01034",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87805771",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af9f02",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Implement mean squared error\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada64d52",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Implement R² score\n",
    "def r2_score(y_true, y_pred):\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true, axis=0)) ** 2, axis=0)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2, axis=0)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5aff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.005\n",
    "epochs = 1000\n",
    "batch_size = 4\n",
    "early_stopping_patience = 50\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ee189",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(X_train.shape[0])\n",
    "    X_shuffled = X_train[indices]\n",
    "    y_shuffled = y_train[indices]\n",
    "    \n",
    "    epoch_losses = []\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_shuffled[i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        Z1 = np.dot(X_batch, W1) + b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = np.dot(A1, W2) + b2\n",
    "        A2 = relu(Z2)\n",
    "        Z3 = np.dot(A2, W3) + b3\n",
    "        y_pred = Z3\n",
    "        \n",
    "        # Compute loss (MSE)\n",
    "        loss = np.mean((y_batch - y_pred) ** 2)\n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "        # Backpropagation\n",
    "        dZ3 = 2 * (y_pred - y_batch) / batch_size\n",
    "        dW3 = np.dot(A2.T, dZ3)\n",
    "        db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
    "        \n",
    "        dA2 = np.dot(dZ3, W3.T)\n",
    "        dZ2 = dA2 * relu_derivative(Z2)\n",
    "        dW2 = np.dot(A1.T, dZ2)\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "        \n",
    "        dA1 = np.dot(dZ2, W2.T)\n",
    "        dZ1 = dA1 * relu_derivative(Z1)\n",
    "        dW1 = np.dot(X_batch.T, dZ1)\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights with learning rate decay\n",
    "        lr = learning_rate / (1 + 0.01 * epoch)\n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "        W3 -= lr * dW3\n",
    "        b3 -= lr * db3\n",
    "    \n",
    "    # Calculate training loss\n",
    "    train_loss = np.mean(epoch_losses)\n",
    "    losses.append(train_loss)\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    Z1_val = np.dot(X_test, W1) + b1\n",
    "    A1_val = relu(Z1_val)\n",
    "    Z2_val = np.dot(A1_val, W2) + b2\n",
    "    A2_val = relu(Z2_val)\n",
    "    y_val_pred = np.dot(A2_val, W3) + b3\n",
    "    val_loss = np.mean((y_test - y_val_pred) ** 2)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_weights = (W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        W1, b1, W2, b2, W3, b3 = best_weights\n",
    "        break\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b210f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "with open('emissions_model_custom.pkl', 'wb') as f:\n",
    "    pickle.dump((W1, b1, W2, b2, W3, b3, scaler_X, scaler_y), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "Z1 = np.dot(X_test, W1) + b1\n",
    "A1 = relu(Z1)\n",
    "Z2 = np.dot(A1, W2) + b2\n",
    "A2 = relu(Z2)\n",
    "y_pred = np.dot(A2, W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82af83",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Denormalize predictions\n",
    "y_pred_original = scaler_y.inverse_transform(y_pred)\n",
    "y_test_original = scaler_y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7f926",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each emission type\n",
    "mse_co2 = mean_squared_error(y_test_original[:, 0], y_pred_original[:, 0])\n",
    "r2_co2 = r2_score(y_test_original[:, 0].reshape(-1, 1), y_pred_original[:, 0].reshape(-1, 1))[0]\n",
    "mse_ch4 = mean_squared_error(y_test_original[:, 1], y_pred_original[:, 1])\n",
    "r2_ch4 = r2_score(y_test_original[:, 1].reshape(-1, 1), y_pred_original[:, 1].reshape(-1, 1))[0]\n",
    "mse_n2o = mean_squared_error(y_test_original[:, 2], y_pred_original[:, 2])\n",
    "r2_n2o = r2_score(y_test_original[:, 2].reshape(-1, 1), y_pred_original[:, 2].reshape(-1, 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d892f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CO2 - MSE: {mse_co2:.4f}, R²: {r2_co2:.4f}\")\n",
    "print(f\"CH4 - MSE: {mse_ch4:.4f}, R²: {r2_ch4:.4f}\")\n",
    "print(f\"N2O - MSE: {mse_n2o:.4f}, R²: {r2_n2o:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612514a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(losses)), losses, label='Training Loss')\n",
    "plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('emissions_training_validation_loss_custom.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05903bbe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to predict emissions for new data\n",
    "def predict_emissions(data, model_file='emissions_model_custom.pkl'):\n",
    "    with open(model_file, 'rb') as f:\n",
    "        W1, b1, W2, b2, W3, b3, scaler_X, scaler_y = pickle.load(f)\n",
    "    \n",
    "    # Preprocess input data\n",
    "    data_scaled = scaler_X.transform(data)\n",
    "    \n",
    "    # Forward pass\n",
    "    Z1 = np.dot(data_scaled, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = relu(Z2)\n",
    "    y_pred = np.dot(A2, W3) + b3\n",
    "    \n",
    "    # Denormalize predictions\n",
    "    return scaler_y.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the original data to visualize\n",
    "all_X = df_processed.values\n",
    "all_X_scaled = scaler_X.transform(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a35635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass for all data\n",
    "Z1_all = np.dot(all_X_scaled, W1) + b1\n",
    "A1_all = relu(Z1_all)\n",
    "Z2_all = np.dot(A1_all, W2) + b2\n",
    "A2_all = relu(Z2_all)\n",
    "all_preds_scaled = np.dot(A2_all, W3) + b3\n",
    "all_preds = scaler_y.inverse_transform(all_preds_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to original dataframe\n",
    "df['Predicted_CO2'] = all_preds[:, 0]\n",
    "df['Predicted_CH4'] = all_preds[:, 1]\n",
    "df['Predicted_N2O'] = all_preds[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dacb15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Plot actual vs predicted for each emission type (using matplotlib instead of seaborn)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3243b8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to create regression plot without seaborn\n",
    "def plot_regression(ax, x, y, title, color):\n",
    "    ax.scatter(x, y, alpha=0.5, color=color)\n",
    "    \n",
    "    # Calculate and plot regression line\n",
    "    coeffs = np.polyfit(x, y, 1)\n",
    "    x_line = np.linspace(min(x), max(x), 100)\n",
    "    y_line = coeffs[0] * x_line + coeffs[1]\n",
    "    ax.plot(x_line, y_line, color='red', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel(f\"Actual {title}\")\n",
    "    ax.set_ylabel(f\"Predicted {title}\")\n",
    "    ax.set_title(f\"{title} Emissions: Actual vs. Predicted (R² = {r2_co2:.4f})\")\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CO2 Plot\n",
    "plot_regression(axes[0], df['CO2'], df['Predicted_CO2'], \"CO2\", 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafbe3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CH4 Plot\n",
    "plot_regression(axes[1], df['CH4'], df['Predicted_CH4'], \"CH4\", 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5dab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N2O Plot\n",
    "plot_regression(axes[2], df['N2O'], df['Predicted_N2O'], \"N2O\", 'purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfe673",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig('emissions_predictions_custom.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b19f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis (using weights as a proxy)\n",
    "feature_names = list(df_processed.columns)\n",
    "importance_w1 = np.sum(np.abs(W1), axis=1)\n",
    "feature_importance = [(feature_names[i], importance_w1[i]) for i in range(len(feature_names))]\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "top_features = feature_importance[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7853c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh([f[0] for f in top_features], [f[1] for f in top_features])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 10 Most Important Features for Emissions Prediction')\n",
    "plt.tight_layout()\n",
    "plt.savefig('emissions_feature_importance_custom.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model training and evaluation complete. Check the generated plots for visualizations.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
